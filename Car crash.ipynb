{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T15:44:36.680659Z","iopub.execute_input":"2021-07-27T15:44:36.681164Z","iopub.status.idle":"2021-07-27T15:44:36.707652Z","shell.execute_reply.started":"2021-07-27T15:44:36.681127Z","shell.execute_reply":"2021-07-27T15:44:36.705134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the required libraries\nimport xml.etree.ElementTree as Xet\ndef xml_toDF(path):\n    \n    cols = [\"timestamp\", \"description\"]\n    rows = []\n\n    xmlparse = Xet.parse(path)\n    root = xmlparse.getroot()\n\n    for i in root:\n        date        = i.find(\"date\").text\n        description = i.find(\"description\").text\n        rows.append({\"timestamp\": date,\"description\": description,})\n\n    return pd.DataFrame(rows, columns=cols)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:36.709327Z","iopub.execute_input":"2021-07-27T15:44:36.709672Z","iopub.status.idle":"2021-07-27T15:44:36.717574Z","shell.execute_reply.started":"2021-07-27T15:44:36.709641Z","shell.execute_reply":"2021-07-27T15:44:36.71624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_full=pd.read_csv('../input/car-crashes-severity-prediction/train.csv')\nprint(train_full.info())\ntest_df = pd.read_csv('../input/car-crashes-severity-prediction/test.csv')\nprint(test_df.info())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:36.748333Z","iopub.execute_input":"2021-07-27T15:44:36.748841Z","iopub.status.idle":"2021-07-27T15:44:36.810877Z","shell.execute_reply.started":"2021-07-27T15:44:36.748809Z","shell.execute_reply":"2021-07-27T15:44:36.809519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom datetime import datetime\ndef generate_holidays_column(df):\n    data = df[['ID' , 'timestamp']]\n    \n    df['day_name'] = pd.to_datetime(df['timestamp']).dt.day_name()\n    \n    df['weekend'] = ~ df['day_name'].isin(['Sunday' , 'Saturday'])\n    df['weekend'].astype(int)\n    d = [datetime.strptime(i.split('.')[0], '%Y-%m-%d %H:%M:%S') for i in data['timestamp'].values]\n    data['timestamp'] = data['timestamp'].apply(lambda x : x.split(\" \")[0])\n    hol = xml_toDF('/kaggle/input/car-crashes-severity-prediction/holidays.xml')\n    x = pd.merge(left=data , right=hol ,  on='timestamp', how = 'left')\n    x['holidays'] = x['description'].isnull().astype(int)\n    \n    x['holidays'] = x['holidays'] * df['weekend']\n#     return x[['ID','holidays']]\n    return x[['ID','holidays']]\n\nhd_df = generate_holidays_column(train_full.copy())\nt1=generate_holidays_column(test_df.copy())\nprint(hd_df.info())\nprint(t1.info())\nprint(t1.head())\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:36.812771Z","iopub.execute_input":"2021-07-27T15:44:36.813117Z","iopub.status.idle":"2021-07-27T15:44:37.017432Z","shell.execute_reply.started":"2021-07-27T15:44:36.813084Z","shell.execute_reply":"2021-07-27T15:44:37.016077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmatrix = hd_df.corr()\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(corrmatrix, vmax=.8, square=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:37.019484Z","iopub.execute_input":"2021-07-27T15:44:37.019805Z","iopub.status.idle":"2021-07-27T15:44:37.322123Z","shell.execute_reply.started":"2021-07-27T15:44:37.019774Z","shell.execute_reply":"2021-07-27T15:44:37.320992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dayOrNight(column , index):\n    df=pd.DataFrame()\n    df['Time_Hour'] = [datetime.strptime(str(d).split(\".\")[0], '%Y-%m-%d %H:%M:%S').hour for d in column]\n    df['dOrN'] =((df['Time_Hour'] >= 6) & (df['Time_Hour'] <= 19)).astype(int)\n    df['rush'] =((df['Time_Hour'] >= 7) & (df['Time_Hour'] <= 9)| (df['Time_Hour'] >= 16) & (df['Time_Hour'] <= 19)).astype(int)    \n    df[\"ID\"] = index\n    return df[[\"ID\" , 'dOrN',\t'rush']]\ndon = dayOrNight(train_full['timestamp'].copy(),train_full.index)\nt2  = dayOrNight(test_df['timestamp'].copy(),test_df['ID'])\n ","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:37.324162Z","iopub.execute_input":"2021-07-27T15:44:37.324519Z","iopub.status.idle":"2021-07-27T15:44:37.491819Z","shell.execute_reply.started":"2021-07-27T15:44:37.324488Z","shell.execute_reply":"2021-07-27T15:44:37.490457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\ndef extract_weather(df): \n    df['timestamp'] = df['timestamp'].apply(lambda x:x[0:-6])\n    weather = pd.read_csv('../input/car-crashes-severity-prediction/weather-sfcsv.csv')\n\n    weather['Year'] = weather['Year'].apply(lambda x : str(x))\n    weather['Month'] = weather['Month'].apply(lambda x : '0'+str(x) if x<10 else str(x))\n    weather['Day'] = weather['Day'].apply(lambda x : '0'+str(x) if x<10 else str(x))\n    weather['Hour'] = weather['Hour'].apply(lambda x : '0'+str(x) if x<10 else str(x))\n    \n    weather_date = weather['Year'].astype(str)+ \"-\" +weather['Month'].astype(str) + \"-\" +weather['Day'].astype(str) + \" \" +weather['Hour'].astype(str)\n    \n    weather=pd.concat([weather,weather_date],axis=1)\n    weather.columns=['Year','Day','Month','Hour','Weather_Condition','Wind_Chill(F)',\n    'Precipitation(in)','Temperature(F)','Humidity(%)','Wind_Speed(mph)','Visibility(mi)','Selected','timestamp']\n    weather=weather.drop_duplicates(subset='timestamp')\n   # print(weather.info())\n    weather=weather.drop(['Year','Day','Month','Hour','Selected','Precipitation(in)'],axis=1)\n    weather_copy=weather.copy()\n   # print(weather.info())\n \n    mergedStuff = pd.merge(df, weather_copy, on='timestamp',how='left')\n    mergedStuff=mergedStuff.drop(['timestamp','Bump','No_Exit','Give_Way','Side','Roundabout'],axis=1)\n    mergedStuff['Stop']=mergedStuff['Stop'].astype(int)\n    mergedStuff['Crossing']=mergedStuff['Crossing'].astype(int)\n    mergedStuff['Junction']=mergedStuff['Junction'].astype(int)\n    mergedStuff['Railway']=mergedStuff['Railway'].astype(int)\n    mergedStuff['Amenity']=mergedStuff['Amenity'].astype(int)\n    mergedStuff['Wind_Chill(F)']=mergedStuff['Wind_Chill(F)'].astype(float)\n    mergedStuff['Wind_Speed(mph)']=mergedStuff['Wind_Speed(mph)'].astype(float)\n    mergedStuff['Visibility(mi)']=mergedStuff['Visibility(mi)'].astype(float)\n    mergedStuff=mergedStuff.fillna(mergedStuff.mean())\n    # create a dict with the order\n    weather_dict = { 'Rain': 200, \n                     'Heavy Rain': 250, \n                     'Light Thunderstorms and Rain': 400,\n                     'Cloudy / Windy':50,\n                     'Fog':80,\n                     'Shallow Fog':40,\n                     'Mostly Cloudy / Windy':70,\n                     'Partly Cloudy / Windy':50,\n                     'Haze':60,\n                     'Smoke':40,\n                     'Partly Cloudy':70,\n                     'Mostly Cloudy':90,\n                     'Overcast':0,\n                     'Fair':0,\n                     'Clear':0,\n                     'Scattered Clouds':15,\n                     'Light Rain':300,\n                     'Fair / Windy':40,\n                     'Cloudy / Windy':30,\n                     'Light Rain / Windy':350,\n                     'Light Drizzle':60,\n                     'Mist':110,\n                     'Rain / Windy':200,\n                     'Patches of Fog':50,\n                     'Squalls':500,\n                     'Fog / Windy':100\n                }\n    # create a copy of the DataFrame\n    df = mergedStuff.copy()\n    # map the order to the column\n    df['Weather_Condition'] = df['Weather_Condition'].map(weather_dict)\n    df['Weather_Condition']=df['Weather_Condition'].fillna(df['Weather_Condition'].mean())\n    return df.copy()\n\ng=extract_weather(train_full.copy())\nt33=extract_weather(test_df.copy())\n\n# import lablel encoder\nfrom sklearn.preprocessing import LabelEncoder\n# create a copy\ndata = g.copy()\nt3=t33.copy()\n# intiate LabelEncoder\nle = LabelEncoder()\ndata['weather_encoded'] = le.fit_transform(data['Weather_Condition'].astype(str))\nt3['weather_encoded']   = le.fit_transform(t3['Weather_Condition'].astype(str))\ndata.info()\nt3.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:37.493446Z","iopub.execute_input":"2021-07-27T15:44:37.493782Z","iopub.status.idle":"2021-07-27T15:44:37.725857Z","shell.execute_reply.started":"2021-07-27T15:44:37.493749Z","shell.execute_reply":"2021-07-27T15:44:37.724139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmatrix = data.corr()\nf, ax = plt.subplots(figsize=(30, 30))\nsns.heatmap(corrmatrix, vmax=.8, square=True,annot=True);","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:37.728114Z","iopub.execute_input":"2021-07-27T15:44:37.728526Z","iopub.status.idle":"2021-07-27T15:44:39.932042Z","shell.execute_reply.started":"2021-07-27T15:44:37.728491Z","shell.execute_reply":"2021-07-27T15:44:39.931108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print()\ndata_with_holidays = pd.merge(data , hd_df , on='ID' )\ndwh=pd.merge(t3 , t1 , on='ID' )\ndwh.info(50)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:39.933213Z","iopub.execute_input":"2021-07-27T15:44:39.933642Z","iopub.status.idle":"2021-07-27T15:44:39.964646Z","shell.execute_reply.started":"2021-07-27T15:44:39.933609Z","shell.execute_reply":"2021-07-27T15:44:39.963324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df=pd.merge(data_with_holidays , don , on='ID' )\n#final_df=final_df.drop(['Weather_Condition'],axis=1)\nprint(dwh.info())\nfinal_test=pd.merge(dwh , t2 , on='ID' )\n#final_test=final_test.drop(['Visibility(mi)','Weather_Condition'],axis=1)\n#final_df=np.nan_to_num(final_df)\nfinal_df.info()\nfinal_test.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:39.967431Z","iopub.execute_input":"2021-07-27T15:44:39.967785Z","iopub.status.idle":"2021-07-27T15:44:40.024681Z","shell.execute_reply.started":"2021-07-27T15:44:39.967752Z","shell.execute_reply":"2021-07-27T15:44:40.023166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmatrix = final_df.corr()\nf, ax = plt.subplots(figsize=(30, 30))\nsns.heatmap(corrmatrix, vmax=.8, square=True,annot=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:40.026687Z","iopub.execute_input":"2021-07-27T15:44:40.027001Z","iopub.status.idle":"2021-07-27T15:44:43.416869Z","shell.execute_reply.started":"2021-07-27T15:44:40.026971Z","shell.execute_reply":"2021-07-27T15:44:43.415586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:43.418458Z","iopub.execute_input":"2021-07-27T15:44:43.418789Z","iopub.status.idle":"2021-07-27T15:44:43.431603Z","shell.execute_reply.started":"2021-07-27T15:44:43.418757Z","shell.execute_reply":"2021-07-27T15:44:43.429905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df['Severity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:43.433232Z","iopub.execute_input":"2021-07-27T15:44:43.433568Z","iopub.status.idle":"2021-07-27T15:44:43.445528Z","shell.execute_reply.started":"2021-07-27T15:44:43.433538Z","shell.execute_reply":"2021-07-27T15:44:43.44382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny=final_df['Severity']\n\ntrain_df, val_df = train_test_split(final_df, test_size=0.2, random_state=42,stratify=y) # Try adding `stratify` here\n\nX_train = train_df.drop(columns=['ID','Severity'])\ny_train = train_df['Severity']\n\nX_val = val_df.drop(columns=[ 'ID','Severity'])\ny_val = val_df['Severity']","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:43.447227Z","iopub.execute_input":"2021-07-27T15:44:43.447679Z","iopub.status.idle":"2021-07-27T15:44:43.470134Z","shell.execute_reply.started":"2021-07-27T15:44:43.447643Z","shell.execute_reply":"2021-07-27T15:44:43.468905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Create an instance of the classifier\nclassifier = RandomForestClassifier(max_depth=2, random_state=0)\n\n# Train the classifier\nclassifier = classifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:43.471595Z","iopub.execute_input":"2021-07-27T15:44:43.471915Z","iopub.status.idle":"2021-07-27T15:44:43.840129Z","shell.execute_reply.started":"2021-07-27T15:44:43.471884Z","shell.execute_reply":"2021-07-27T15:44:43.838399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The accuracy of the classifier on the validation set is \", (classifier.score(X_val, y_val)))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:43.841708Z","iopub.execute_input":"2021-07-27T15:44:43.842663Z","iopub.status.idle":"2021-07-27T15:44:43.878125Z","shell.execute_reply.started":"2021-07-27T15:44:43.842606Z","shell.execute_reply":"2021-07-27T15:44:43.876422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test.drop(['ID'] , axis=1 , inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:43.880051Z","iopub.execute_input":"2021-07-27T15:44:43.880431Z","iopub.status.idle":"2021-07-27T15:44:43.888007Z","shell.execute_reply.started":"2021-07-27T15:44:43.880398Z","shell.execute_reply":"2021-07-27T15:44:43.887042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = classifier.predict(final_test)\noutput = pd.DataFrame(predict , columns=['Severity'] , index = test_full['ID'])\noutput.to_csv('submission.csv')\noutput","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:44:43.889443Z","iopub.execute_input":"2021-07-27T15:44:43.890006Z","iopub.status.idle":"2021-07-27T15:44:43.947681Z","shell.execute_reply.started":"2021-07-27T15:44:43.889973Z","shell.execute_reply":"2021-07-27T15:44:43.946574Z"},"trusted":true},"execution_count":null,"outputs":[]}]}